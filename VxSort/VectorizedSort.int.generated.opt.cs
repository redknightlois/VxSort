/////////////////////////////////////////////////////////////////////////////
////
// This file was auto-generated by a tool at 2021-10-06 17:54:26
//
// It is recommended you DO NOT directly edit this file but instead edit
// the code-generator that generated this source file instead.
/////////////////////////////////////////////////////////////////////////////
    
    using System;
    using System.Diagnostics;
    using System.Runtime.CompilerServices;
    using System.Runtime.InteropServices;
    using System.Runtime.Intrinsics;
    using System.Runtime.Intrinsics.X86;
    using static System.Runtime.Intrinsics.X86.Avx;
    using static System.Runtime.Intrinsics.X86.Avx2;
    using static System.Runtime.Intrinsics.X86.Sse2;
    using static System.Runtime.Intrinsics.X86.Sse41;
    using static System.Runtime.Intrinsics.X86.Sse42;
    using static System.Runtime.Intrinsics.X86.Popcnt.X64;
    using static System.Runtime.Intrinsics.X86.Popcnt;
using static VxSort.VectorExtensions;
    
    namespace VxSort
    {
        using V = Vector256<int>;
        using VStatic = Vector256;
        unsafe partial struct VectorizedSort<MT, MP>
        {


        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void partition_block_with_compress(in V dataVec, in V p, byte* pBase, ref int* left, ref int* right)
        {
            throw new NotSupportedException();
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void partition_block(V dataVec, V p, byte* pBase, ref int* left, ref int* right, bool compressWrites = false)
        {
            if (compressWrites)
            {
                partition_block_with_compress(dataVec, p, pBase, ref left, ref right);
            }
            else
            {
                partition_block_without_compress(dataVec, p, pBase, ref left, ref right);
            }
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void partition_block_without_compress(V dataVec, V p, byte* pBase, ref int* left, ref int* right)
        {
            var mask = (ulong)(uint)MoveMask(CompareGreaterThan(dataVec, p).AsSingle()); // default(MT).get_cmpgt_mask(dataVec, p);

            // Looks kinda silly, the (ulong) (uint) thingy right?
            // Well, it's making a yucky lemonade out of lemons is what it is.
            // This is a crappy way of making the jit generate slightly less worse code
            // due to: https://github.com/dotnet/runtime/issues/431#issuecomment-568280829
            // To summarize: VMOVMASK is mis-understood as a 32-bit write by the CoreCLR 3.x JIT.
            // It's really a 64 bit write in 64 bit mode, in other words, it clears the entire register.
            // Again, the JIT *should* be aware that the destination register just had it's top 32 bits cleared.
            // It doesn't.
            // This causes a variety of issues, here it's that GetBytePermutation* method is generated
            // with suboptimal x86 code (see above issue/comment).
            // By forcefully clearing the 32-top bits by casting to ulong, we "help" the JIT further down the road
            // and the rest of the code is generated more cleanly.
            // In other words, until the issue is resolved we "pay" with a 2-byte instruction for this useless cast
            // But this helps the JIT generate slightly better code below (saving 3 bytes).
            var table = ConvertToVector256Int32(LoadVector128(pBase + mask * 8));
            var maskedDataVec = PermuteVar8x32(dataVec, table); //default(MT).partition_vector(dataVec, pBase, mask);

            // By "delaying" the PopCount to this stage, it is highly likely (I don't know why, I just know it is...)
            // that the JIT will emit a POPCNT X,X instruction, where X is now both the source and the destination
            // for PopCount. This means that there is no need for clearing the destination register (it would even be
            // an error to do so). This saves about two bytes in the instruction stream.
            var pc = -(long)(int)PopCount(mask); // default(MT).popcnt(mask);

            Store(left, maskedDataVec); // default(MT).store_vec(left, maskedDataVec);
            Store(right, maskedDataVec); // default(MT).store_vec(right, maskedDataVec);
                                         
            // I comfortably ignored having negated the PopCount result after casting to (long)
            // The reasoning behind this is that be storing the PopCount as a negative
            // while also expressing the pointer bumping (next two lines) in this very specific form that
            // it is expressed: a summation of two variables with an optional constant (that CAN be negative)
            // We are allowing the JIT to encode this as two LEA opcodes in x64: https://www.felixcloutier.com/x86/lea
            // This saves a considerable amount of space in the instruction stream, which are then exploded
            // when this block is unrolled. All in all this is has a very clear benefit in perf while decreasing code
            // size.
            // TODO: Currently the entire sorting operation generates a right-hand popcount that needs to be negated
            //       If/When I re-write it to do left-hand comparison/pop-counting we can save another two bytes
            //       for the negation operation, which will also do its share to speed things up while lowering
            //       the native code size, yay for future me!
            right = right + pc;
            left = left + pc + V.Count; // default(MP).N;
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void align_vectorized(
                int* left, int* right,
                int leftAlign, int rightAlign,
                in V p,
                byte* pBase,
                ref int* readLeft, ref int* readRight,
                ref int* tmpStartLeft, ref int* tmpLeft,
                ref int* tmpStartRight, ref int* tmpRight, bool compressWrites = false)
        {
            // PERF: CompressWrite support is been treated as a constant because we make sure the caller
            //       treats that parameter already as a constant @ JIT time causing a cascade.

            int N = V.Count;

            var rai = ~((rightAlign - 1) >> 31);
            var lai = leftAlign >> 31;
            var preAlignedLeft = left + leftAlign;
            var preAlignedRight = right + rightAlign - N;

            // Alignment with vectorization is tricky, so read carefully before changing code:
            // 1. We load data, which we might need to align, if the alignment hints
            //    mean pre-alignment (or overlapping alignment)
            // 2. We partition and store in the following order:
            //    a) right-portion of right vector to the right-side
            //    b) left-portion of left vector to the right side
            //    c) at this point one-half of each partitioned vector has been committed
            //       back to memory.
            //    d) we advance the right write (tmpRight) pointer by how many elements
            //       were actually needed to be written to the right hand side
            //    e) We write the right portion of the left vector to the right side
            //       now that its write position has been updated

            var RT0 = LoadAlignedVector256(preAlignedRight); // default(MT).load_vec<V>(preAlignedRight, out var RT0);
            var LT0 = LoadAlignedVector256(preAlignedLeft); // default(MT).load_vec<V>(preAlignedLeft, out var LT0);
            var rtMask = (uint)MoveMask(CompareGreaterThan(RT0, p).AsSingle()); //default(MT).get_cmpgt_mask(RT0, p);
            var ltMask = (uint)MoveMask(CompareGreaterThan(LT0, p).AsSingle()); //default(MT).get_cmpgt_mask(LT0, p);
            var rtPopCountRightPart = Math.Max(PopCount(rtMask), (uint)rightAlign); // Math.Max(default(MT).popcnt(rtMask), rightAlign);            
            var rtPopCountLeftPart = N - rtPopCountRightPart;
            var ltPopCountRightPart = PopCount(ltMask); // default(MT).popcnt(ltMask);
            var ltPopCountLeftPart = N - ltPopCountRightPart;

            if (compressWrites)
            {
                throw new NotImplementedException();
            }
            else
            {
                RT0 = PermuteVar8x32(RT0, ConvertToVector256Int32(LoadVector128(pBase + rtMask * 8))); // default(MT).partition_vector(RT0, pBase, rtMask)
                Store(tmpRight, RT0); //default(MT).store_vec(tmpRight, RT0);

                LT0 = PermuteVar8x32(LT0, ConvertToVector256Int32(LoadVector128(pBase + ltMask * 8))); // default(MT).partition_vector(LT0, pBase, ltMask);                
                Store(tmpLeft, LT0); //default(MT).store_vec(tmpLeft, LT0);

                tmpRight -= rtPopCountRightPart & rai;
                readRight += (rightAlign - N) & rai;

                Store(tmpRight, LT0); // default(MT).store_vec(tmpRight, LT0);
                tmpRight -= ltPopCountRightPart & lai;
                tmpLeft += ltPopCountLeftPart & lai;
                tmpStartLeft += -leftAlign & lai;
                readLeft += (leftAlign + N) & lai;

                Store(tmpLeft, RT0); // default(MT).store_vec(tmpLeft, RT0);
                tmpLeft += rtPopCountLeftPart & rai;
                tmpStartRight -= rightAlign & rai;
            }
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void LoadAndPartitionVectors(int* dataPtr, V P, byte* pBase, ref int* writeLeftPtr, ref int* writeRightPtr, int unroll = 1, bool compressWrites = false)
        {
            // PERF: Unroll and CompressWrite support is been treated as a constant because we make sure the caller
            //       treats that parameter already as a constant @ JIT time causing a cascade.

            var N = V.Count; // Treated as constant @ JIT time

            // We are not going to initialize these definitions.
            Unsafe.SkipInit(out V d01);
            Unsafe.SkipInit(out V d02);
            Unsafe.SkipInit(out V d03);
            Unsafe.SkipInit(out V d04);
            Unsafe.SkipInit(out V d05);
            Unsafe.SkipInit(out V d06);
            Unsafe.SkipInit(out V d07);
            Unsafe.SkipInit(out V d08);
            Unsafe.SkipInit(out V d09);
            Unsafe.SkipInit(out V d10);
            Unsafe.SkipInit(out V d11);
            Unsafe.SkipInit(out V d12);            

            switch (unroll)
            {
                case 12: d12 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 12))); goto case 11;
                case 11: d11 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 11))); goto case 10;
                case 10: d10 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 10))); goto case 9;
                case 9: d09 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 9))); goto case 8;
                case 8: d08 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 8))); goto case 7;
                case 7: d07 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 7))); goto case 6;
                case 6: d06 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 6))); goto case 5;
                case 5: d05 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 5))); goto case 4;
                case 4: d04 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 4))); goto case 3;
                case 3: d03 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 3))); goto case 2;
                case 2: d02 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 2))); goto case 1;
                case 1: d01 = LoadAlignedVector256((int*)(dataPtr + N * (unroll - 1))); break;
            }

            switch (unroll)
            {
                case 12: partition_block(d12, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 11;
                case 11: partition_block(d11, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 10;
                case 10: partition_block(d10, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 9;
                case 9: partition_block(d09, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 8;
                case 8: partition_block(d08, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 7;
                case 7: partition_block(d07, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 6;
                case 6: partition_block(d06, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 5;
                case 5: partition_block(d05, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 4;
                case 4: partition_block(d04, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 3;
                case 3: partition_block(d03, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 2;
                case 2: partition_block(d02, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); goto case 1;
                case 1: partition_block(d01, P, pBase, ref writeLeftPtr, ref writeRightPtr, compressWrites); break;
            }
        }


        private static readonly MP Parameters = default(MP);
        private static readonly MT Traits = default(MT);

        [SkipLocalsInit]
        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static int* vectorized_partition_impl(int* left, int* right, long hint, int tmpSizeInElements, int unroll = 1, bool useCompressWrites = false)
        {
            int N = V.Count;

            Debug.Assert(right - left >= default(MP).SMALL_SORT_THRESHOLD_ELEMENTS);
            Debug.Assert(((long)left & default(MP).ELEMENT_ALIGN) == 0);
            Debug.Assert(((long)right & default(MP).ELEMENT_ALIGN) == 0);

            // Vectorized double-pumped (dual-sided) partitioning:
            // We start with picking a pivot using the media-of-3 "method"
            // Once we have sensible pivot stored as the last element of the array
            // We process the array from both ends.
            //
            // To get this rolling, we first read 2 Vector256 elements from the left and
            // another 2 from the right, and store them in some temporary space in order
            // to leave enough "space" inside the vector for storing partitioned values.
            // Why 2 from each side? Because we need n+1 from each side where n is the
            // number of Vector256 elements we process in each iteration... The
            // reasoning behind the +1 is because of the way we decide from *which* side
            // to read, we may end up reading up to one more vector from any given side
            // and writing it in its entirety to the opposite side (this becomes
            // slightly clearer when reading the code below...) Conceptually, the bulk
            // of the processing looks like this after clearing out some initial space
            // as described above:

            // [.............................................................................]
            //  ^wl          ^rl                                               rr^ wr^
            // Where:
            // wl = writeLeft
            // rl = readLeft
            // rr = readRight
            // wr = writeRight

            // In every iteration, we select what side to read from based on how much
            // space is left between head read/write pointer on each side...
            // We read from where there is a smaller gap, e.g. that side
            // that is closer to the unfortunate possibility of its write head
            // overwriting its read head... By reading from THAT side, we're ensuring
            // this does not happen

            // An additional unfortunate complexity we need to deal with is that the
            // right pointer must be decremented by another Vector256<T>.Count elements
            // Since the Load/Store primitives obviously accept start addresses
            var pivot = *right;

            // We do this here just in case we need to pre-align to the right
            // We end up
            *right = int.MaxValue;

            // Broadcast the selected pivot
            var P = VStatic.Create(pivot); // default(MT).broadcast<int, V>(pivot);

            var readLeft = left;
            var readRight = right;

            var temp = stackalloc int[tmpSizeInElements];
            //Span<int> tempSpan = new Span<int>(temp, default(MP).PARTITION_TMP_SIZE_IN_ELEMENTS);

            var tmpStartLeft = temp;
            var tmpLeft = tmpStartLeft;
            var tmpStartRight = temp + tmpSizeInElements;
            var tmpRight = tmpStartRight;

            tmpRight -= N;

            //Console.WriteLine("Values:[{0}]", string.Join(',', new Span<int>(left, (int)(right - left)).ToArray()));

            var leftAlign = unchecked((int)(hint & 0xFFFFFFFF));
            var rightAlign = unchecked((int)(hint >> 32));

            var pBase = (byte*)Unsafe.AsPointer(ref MemoryMarshal.GetReference(perm_table_32));

            // the read heads always advance by 8 elements, or 32 bytes,
            // We can spend some extra time here to align the pointers
            // so they start at a cache-line boundary
            // Once that happens, we can read with Avx.LoadAlignedVector256
            // And also know for sure that our reads will never cross cache-lines
            // Otherwise, 50% of our AVX2 Loads will need to read from two cache-lines
            align_vectorized(left, right,
                leftAlign, rightAlign, P, pBase,
                ref readLeft, ref readRight,
                ref tmpStartLeft, ref tmpLeft, ref tmpStartRight, ref tmpRight, compressWrites: useCompressWrites);

            if (leftAlign > 0)
            {
                tmpRight += N;
                readLeft = align_left_scalar_uncommon(readLeft, pivot, ref tmpLeft, ref tmpRight);
                tmpRight -= N;
            }

            if (rightAlign < 0)
            {
                tmpRight += N;
                readRight = align_right_scalar_uncommon(readRight, pivot, ref tmpLeft, ref tmpRight);
                tmpRight -= N;
            }

            Debug.Assert(((ulong)readLeft & Sort.ALIGN_MASK) == 0);
            Debug.Assert(((ulong)readRight & Sort.ALIGN_MASK) == 0);

            Debug.Assert((((ulong)readRight - (ulong)readLeft) % Sort.ALIGN) == 0);
            Debug.Assert((readRight - readLeft) >= unroll * 2);

            // From now on, we are fully aligned
            // and all reading is done in full vector units

            var readLeftV = readLeft;
            var readRightV = readRight;            

            LoadAndPartitionVectors(readLeftV, P, pBase, ref tmpLeft, ref tmpRight, unroll: unroll, compressWrites: useCompressWrites);
            LoadAndPartitionVectors(readRightV - unroll * N, P, pBase, ref tmpLeft, ref tmpRight, unroll: unroll, compressWrites: useCompressWrites);

            tmpRight += N;

            //Console.WriteLine("TempL:[{string.Join(',', new Span<int>(tmpStartLeft, (int)(tmpLeft - tmpStartLeft)).ToArray()}]");
            //Console.WriteLine("TempR:[{string.Join(',', new Span<int>(tmpRight, (int)(tmpStartRight - tmpRight)).ToArray()}]");

            // Adjust for the reading that was made above
            readLeftV += N * unroll;
            readRightV -= N * unroll * 2;

            int* nextPtr;

            var writeLeft = left;
            var writeRight = right - N;

            while (readLeftV < readRightV)
            {
                if (writeRight - readRightV < (2 * (unroll * N) - N))
                {
                    nextPtr = readRightV;
                    readRightV -= N * unroll;
                }
                else
                {
                    // PERF: Ensure that JIT neven emits cmov here.
                    nextPtr = readLeftV;
                    readLeftV += N * unroll;
                }

                LoadAndPartitionVectors(nextPtr, P, pBase, ref writeLeft, ref writeRight, unroll: unroll, compressWrites: useCompressWrites);
            }

            readRightV += N * (unroll - 1);

            //Console.WriteLine("WL:[{string.Join(',', new Span<int>(left, (int)(writeLeft - left)).ToArray()}]");
            //Console.WriteLine("WR:[{string.Join(',', new Span<int>(writeRight, (int)(right - writeRight)).ToArray()}]");

            while (readLeftV <= readRightV)
            {
                if (writeRight - readRightV < N)
                {
                    nextPtr = readRightV;
                    readRightV -= N;
                }
                else
                {
                    // PERF: Ensure that JIT never emits cmov here.
                    nextPtr = readLeftV;
                    readLeftV += N;
                }

                LoadAndPartitionVectors(nextPtr, P, pBase, ref writeLeft, ref writeRight, compressWrites: useCompressWrites);
            }

            //Console.WriteLine("WL:[{string.Join(',', new Span<int>(left, (int)(writeLeft - left)).ToArray()}]");
            //Console.WriteLine("WR:[{string.Join(',', new Span<int>(writeRight, (int)(right - writeRight)).ToArray()}]");

            // 3. Copy-back the 4 registers + remainder we partitioned in the beginning
            var leftTmpSize = tmpLeft - tmpStartLeft;
            Unsafe.CopyBlockUnaligned(writeLeft, tmpStartLeft, (uint)(leftTmpSize * sizeof(int)));
            writeLeft += leftTmpSize;
            //Console.WriteLine("WL:[{string.Join(',', new Span<int>(left, (int)(writeLeft - left)).ToArray()}]");
            var rightTmpSize = tmpStartRight - tmpRight;
            Unsafe.CopyBlockUnaligned(writeLeft, tmpRight, (uint)(rightTmpSize * sizeof(int)));
            //writeLeft += rightTmpSize;

            //Console.WriteLine("W:[{string.Join(',', new Span<int>(left, (int)(right - left)).ToArray()}]");
            // Shove to pivot back to the boundary
            *right = *writeLeft;
            *writeLeft++ = pivot;

            //Console.WriteLine("W:[{string.Join(',', new Span<int>(left, (int)(right - left)).ToArray()}]", ));

            Debug.Assert(writeLeft > left);
            Debug.Assert(writeLeft <= right + 1);

            return writeLeft;
        }
        

        };
    }
    
