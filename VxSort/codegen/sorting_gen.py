#!/usr/bin/env python3
#
# This is a tool to generate the vectorized sorter code that is used for big arrays.
#
# usage: sorting_gen.py [-h] [--output-dir OUTPUT_DIR]
#
#
import argparse
import os

from configuration import Configuration
from sorting_isa import SortingISA
from sorting_avx2 import AVX2SortingISA

from enum import Enum
from abc import ABC, ABCMeta, abstractmethod
from datetime import datetime

SortingISA.register(AVX2SortingISA)

# class SortGenerator:
#     def __init__(self, type):
#
#         self.type = type
#         self.type_map = {
#             "int": "Int32",
#             "uint": "Int32",
#             "float": "Int32",
#             "long": "Int64",
#             "ulong": "Int64",
#             "double": "Int64",
#         }
#
#     @staticmethod
#     def supported_types():
#         return ['int', 'uint', 'long', 'ulong', 'float', 'double']
#
#
#     def autogenerated_blabber(self):
#         return f"""/////////////////////////////////////////////////////////////////////////////
# ////
# // This file was auto-generated by a tool at {datetime.now().strftime("%F %H:%M:%S")}
# //
# // It is recommended you DO NOT directly edit this file but instead edit
# // the code-generator that generated this source file instead.
# /////////////////////////////////////////////////////////////////////////////"""
#
#     def generate_prologue(self, f):
#         t = self.type
#         s = f"""{self.autogenerated_blabber()}
#
#     using System;
#     using System.Diagnostics;
#     using System.Runtime.CompilerServices;
#     using System.Runtime.InteropServices;
#     using System.Runtime.Intrinsics;
#     using static System.Runtime.Intrinsics.X86.Avx;
#     using static System.Runtime.Intrinsics.X86.Avx2;
#     using static System.Runtime.Intrinsics.X86.Sse2;
#     using static System.Runtime.Intrinsics.X86.Sse41;
#     using static System.Runtime.Intrinsics.X86.Sse42;
#     using static VxSort.VectorExtensions;
#
#     namespace VxSort
#     {{
#         using V = Vector256<{self.type}>;
#         unsafe partial struct VectorizedSort<MT, MP>
#         {{
#     """
#         print(s, file=f)
#
#     def generate_epilogue(self, f):
#         s = f"""
#         }};
#     }}
#     """
#         print(s, file=f)
#
#     def generate_partition_block(self, f):
#         t = self.type
#         s = f"""
#             private static void partition_block_with_compress(in V dataVec, in V p, ref {t}* left, ref {t}* right)
#             {{
#                 throw new NotSupportedException();
#             }}
#
#             [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
#             private static void partition_block(in V dataVec, in V p, ref {t}* left, ref {t}* right)
#             {{
#                 if (default(MT).supports_compress_writes)
#                 {{
#                     partition_block_with_compress(in dataVec, in p, ref left, ref right);
#                 }}
#                 else
#                 {{
#                     partition_block_without_compress(in dataVec, in p, ref left, ref right);
#                 }}
#             }}
#
#             [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
#             private static void partition_block_without_compress(in V dataVec, in V p, ref {t}* left, ref {t}* right)
#             {{
#                 var mask = default(MT).get_cmpgt_mask(dataVec, p);
#                 var maskedDataVec = default(MT).partition_vector(dataVec, mask);
#                 default(MT).store_vec(left, maskedDataVec);
#                 default(MT).store_vec(right, maskedDataVec);
#                 var popCount = -default(MT).popcnt(mask);
#                 right += popCount;
#                 left += popCount + default(MP).N; // TODO: V.Count
#             }}
#         """
#         print(s, file=f)
#
#     def generate_align_vectorized(self, f):
#         t = self.type
#         s = f"""
#             [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
#             private static void align_vectorized(
#                 {t}* left, {t}* right,
#                 int leftAlign, int rightAlign,
#                 in V p,
#                 ref {t}* readLeft, ref {t}* readRight,
#                 ref {t}* tmpStartLeft, ref {t}* tmpLeft,
#                 ref {t}* tmpStartRight, ref {t}* tmpRight)
#             {{
#                 int N = V.Count;
#
#                 var rai = ~((rightAlign - 1) >> 31);
#                 var lai = leftAlign >> 31;
#                 var preAlignedLeft = left + leftAlign;
#                 var preAlignedRight = right + rightAlign - N;
#
#                 // Alignment with vectorization is tricky, so read carefully before changing code:
#                 // 1. We load data, which we might need to align, if the alignment hints
#                 //    mean pre-alignment (or overlapping alignment)
#                 // 2. We partition and store in the following order:
#                 //    a) right-portion of right vector to the right-side
#                 //    b) left-portion of left vector to the right side
#                 //    c) at this point one-half of each partitioned vector has been committed
#                 //       back to memory.
#                 //    d) we advance the right write (tmpRight) pointer by how many elements
#                 //       were actually needed to be written to the right hand side
#                 //    e) We write the right portion of the left vector to the right side
#                 //       now that its write position has been updated
#
#                 default(MT).load_vec<V>(preAlignedRight, out var RT0);
#                 default(MT).load_vec<V>(preAlignedLeft, out var LT0);
#                 var rtMask = default(MT).get_cmpgt_mask(RT0, p);
#                 var ltMask = default(MT).get_cmpgt_mask(LT0, p);
#                 var rtPopCountRightPart = Math.Max(default(MT).popcnt(rtMask), rightAlign);
#                 var ltPopCountRightPart = default(MT).popcnt(ltMask);
#                 var rtPopCountLeftPart = N - rtPopCountRightPart;
#                 var ltPopCountLeftPart = N - ltPopCountRightPart;
#
#                 if (default(MT).supports_compress_writes)
#                 {{
#                     throw new NotImplementedException();
#                 }}
#                 else
#                 {{
#                     RT0 = default(MT).partition_vector(RT0, rtMask);
#                     LT0 = default(MT).partition_vector(LT0, ltMask);
#                     default(MT).store_vec(tmpRight, RT0);
#                     default(MT).store_vec(tmpLeft, LT0);
#
#                     tmpRight -= rtPopCountRightPart & rai;
#                     readRight += (rightAlign - N) & rai;
#
#                     default(MT).store_vec(tmpRight, LT0);
#                     tmpRight -= ltPopCountRightPart & lai;
#
#                     tmpLeft += ltPopCountLeftPart & lai;
#                     tmpStartLeft += -leftAlign & lai;
#                     readLeft += (leftAlign + N) & lai;
#
#                     default(MT).store_vec(tmpLeft, RT0);
#                     tmpLeft += rtPopCountLeftPart & rai;
#                     tmpStartRight -= rightAlign & rai;
#                 }}
#             }}
# """
#         print(s, file=f)
#
#     def generate_partitioning_impl(self, f):
#         t = self.type
#         s = f"""
#         [SkipLocalsInit]
#         private static {t}* vectorized_partition_impl<TUnroll>({t}* left, {t}* right, long hint)
#             where TUnroll : struct, IUnrollOptions
#         {{
#             int N = V.Count;
#             int Unroll = default(TUnroll).Unroll;
#
#             Debug.Assert(right - left >= default(MP).SMALL_SORT_THRESHOLD_ELEMENTS);
#             Debug.Assert(((long)left & default(MP).ELEMENT_ALIGN) == 0);
#             Debug.Assert(((long)right & default(MP).ELEMENT_ALIGN) == 0);
#
#             // Vectorized double-pumped (dual-sided) partitioning:
#             // We start with picking a pivot using the media-of-3 "method"
#             // Once we have sensible pivot stored as the last element of the array
#             // We process the array from both ends.
#             //
#             // To get this rolling, we first read 2 Vector256 elements from the left and
#             // another 2 from the right, and store them in some temporary space in order
#             // to leave enough "space" inside the vector for storing partitioned values.
#             // Why 2 from each side? Because we need n+1 from each side where n is the
#             // number of Vector256 elements we process in each iteration... The
#             // reasoning behind the +1 is because of the way we decide from *which* side
#             // to read, we may end up reading up to one more vector from any given side
#             // and writing it in its entirety to the opposite side (this becomes
#             // slightly clearer when reading the code below...) Conceptually, the bulk
#             // of the processing looks like this after clearing out some initial space
#             // as described above:
#
#             // [.............................................................................]
#             //  ^wl          ^rl                                               rr^ wr^
#             // Where:
#             // wl = writeLeft
#             // rl = readLeft
#             // rr = readRight
#             // wr = writeRight
#
#             // In every iteration, we select what side to read from based on how much
#             // space is left between head read/write pointer on each side...
#             // We read from where there is a smaller gap, e.g. that side
#             // that is closer to the unfortunate possibility of its write head
#             // overwriting its read head... By reading from THAT side, we're ensuring
#             // this does not happen
#
#             // An additional unfortunate complexity we need to deal with is that the
#             // right pointer must be decremented by another Vector256<T>.Count elements
#             // Since the Load/Store primitives obviously accept start addresses
#             var pivot = *right;
#
#             // We do this here just in case we need to pre-align to the right
#             // We end up
#             *right = int.MaxValue;
#
#             // Broadcast the selected pivot
#             var P = default(MT).broadcast<{t}, V>(pivot);
#
#             var readLeft = left;
#             var readRight = right;
#
#             var temp = stackalloc {t}[default(MP).PARTITION_TMP_SIZE_IN_ELEMENTS];
#             Span<{t}> tempSpan = new Span<{t}>(temp, default(MP).PARTITION_TMP_SIZE_IN_ELEMENTS);
#
#             var tmpStartLeft = temp;
#             var tmpLeft = tmpStartLeft;
#             var tmpStartRight = temp + default(MP).PARTITION_TMP_SIZE_IN_ELEMENTS;
#             var tmpRight = tmpStartRight;
#
#             tmpRight -= N;
#
#             //Console.WriteLine("Values:[{{0}}]", string.Join(',', new Span<{t}>(left, (int)(right - left)).ToArray()));
#
#             var leftAlign = unchecked((int)(hint & 0xFFFFFFFF));
#             var rightAlign = unchecked((int)(hint >> 32));
#
#             // the read heads always advance by 8 elements, or 32 bytes,
#             // We can spend some extra time here to align the pointers
#             // so they start at a cache-line boundary
#             // Once that happens, we can read with Avx.LoadAlignedVector256
#             // And also know for sure that our reads will never cross cache-lines
#             // Otherwise, 50% of our AVX2 Loads will need to read from two cache-lines
#             align_vectorized(left, right,
#                 leftAlign, rightAlign, P,
#                 ref readLeft, ref readRight,
#                 ref tmpStartLeft, ref tmpLeft, ref tmpStartRight, ref tmpRight);
#
#             if (leftAlign > 0)
#             {{
#                 tmpRight += N;
#                 readLeft = align_left_scalar_uncommon(readLeft, pivot, ref tmpLeft, ref tmpRight);
#                 tmpRight -= N;
#             }}
#
#             if (rightAlign < 0)
#             {{
#                 tmpRight += N;
#                 readRight = align_right_scalar_uncommon(readRight, pivot, ref tmpLeft, ref tmpRight);
#                 tmpRight -= N;
#             }}
#
#             Debug.Assert(((ulong)readLeft & Sort.ALIGN_MASK) == 0);
#             Debug.Assert(((ulong)readRight & Sort.ALIGN_MASK) == 0);
#
#             Debug.Assert((((ulong)readRight - (ulong)readLeft) % Sort.ALIGN) == 0);
#             Debug.Assert((readRight - readLeft) >= Unroll * 2);
#
#             // From now on, we are fully aligned
#             // and all reading is done in full vector units
#
#             var readLeftV = readLeft;
#             var readRightV = readRight;
#
#             for (var u = 0; u < Unroll; u++)
#             {{
#                 //Console.WriteLine("Lidx:[{{(int)(readLeftV + (N * u) - left)}}|{{*(readLeftV + (N * u))}}], Ridx:[{{(int)(readRightV - N * (u + 1) - left)}}|{{*(readRightV - N * (u + 1))}}]");
#                 default(MT).load_vec(readLeftV + (N * u), out V dl);
#                 default(MT).load_vec(readRightV - N * (u + 1), out V dr);
#                 partition_block(in dl, in P, ref tmpLeft, ref tmpRight);
#                 partition_block(in dr, in P, ref tmpLeft, ref tmpRight);
#             }}
#
#             tmpRight += N;
#
#             //Console.WriteLine("TempL:[{{string.Join(',', new Span<int>(tmpStartLeft, (int)(tmpLeft - tmpStartLeft)).ToArray()}}]");
#             //Console.WriteLine("TempR:[{{string.Join(',', new Span<int>(tmpRight, (int)(tmpStartRight - tmpRight)).ToArray()}}]");
#
#             // Adjust for the reading that was made above
#             readLeftV += N * Unroll;
#             readRightV -= N * Unroll * 2;
#
#             {t}* nextPtr;
#
#             var writeLeft = left;
#             var writeRight = right - N;
#
#             while (readLeftV < readRightV)
#             {{
#                 if (writeRight - readRightV < (2 * (Unroll * N) - N))
#                 {{
#                     nextPtr = readRightV;
#                     readRightV -= N * Unroll;
#                 }}
#                 else
#                 {{
#                     // PERF: Ensure that JIT neven emits cmov here.
#                     nextPtr = readLeftV;
#                     readLeftV += N * Unroll;
#                 }}
#
#                 // We are not going to initialize these definitions.
#                 Unsafe.SkipInit(out V d01);
#                 Unsafe.SkipInit(out V d02);
#                 Unsafe.SkipInit(out V d03);
#                 Unsafe.SkipInit(out V d04);
#                 Unsafe.SkipInit(out V d05);
#                 Unsafe.SkipInit(out V d06);
#                 Unsafe.SkipInit(out V d07);
#                 Unsafe.SkipInit(out V d08);
#                 Unsafe.SkipInit(out V d09);
#                 Unsafe.SkipInit(out V d10);
#                 Unsafe.SkipInit(out V d11);
#                 Unsafe.SkipInit(out V d12);
#
#                 switch (Unroll)
#                 {{
#                     case 12: default(MT).load_vec(nextPtr + N * (Unroll - 12), out d12); goto case 11;
#                     case 11: default(MT).load_vec(nextPtr + N * (Unroll - 11), out d11); goto case 10;
#                     case 10: default(MT).load_vec(nextPtr + N * (Unroll - 10), out d10); goto case 9;
#                     case 9: default(MT).load_vec(nextPtr + N * (Unroll - 9), out d09); goto case 8;
#                     case 8: default(MT).load_vec(nextPtr + N * (Unroll - 8), out d08); goto case 7;
#                     case 7: default(MT).load_vec(nextPtr + N * (Unroll - 7), out d07); goto case 6;
#                     case 6: default(MT).load_vec(nextPtr + N * (Unroll - 6), out d06); goto case 5;
#                     case 5: default(MT).load_vec(nextPtr + N * (Unroll - 5), out d05); goto case 4;
#                     case 4: default(MT).load_vec(nextPtr + N * (Unroll - 4), out d04); goto case 3;
#                     case 3: default(MT).load_vec(nextPtr + N * (Unroll - 3), out d03); goto case 2;
#                     case 2: default(MT).load_vec(nextPtr + N * (Unroll - 2), out d02); goto case 1;
#                     case 1: default(MT).load_vec(nextPtr + N * (Unroll - 1), out d01); break;
#                 }}
#
#                 switch (Unroll)
#                 {{
#                     case 12: partition_block(in d12, in P, ref writeLeft, ref writeRight); goto case 11;
#                     case 11: partition_block(in d11, in P, ref writeLeft, ref writeRight); goto case 10;
#                     case 10: partition_block(in d10, in P, ref writeLeft, ref writeRight); goto case 9;
#                     case 9: partition_block(in d09, in P, ref writeLeft, ref writeRight); goto case 8;
#                     case 8: partition_block(in d08, in P, ref writeLeft, ref writeRight); goto case 7;
#                     case 7: partition_block(in d07, in P, ref writeLeft, ref writeRight); goto case 6;
#                     case 6: partition_block(in d06, in P, ref writeLeft, ref writeRight); goto case 5;
#                     case 5: partition_block(in d05, in P, ref writeLeft, ref writeRight); goto case 4;
#                     case 4: partition_block(in d04, in P, ref writeLeft, ref writeRight); goto case 3;
#                     case 3: partition_block(in d03, in P, ref writeLeft, ref writeRight); goto case 2;
#                     case 2: partition_block(in d02, in P, ref writeLeft, ref writeRight); goto case 1;
#                     case 1: partition_block(in d01, in P, ref writeLeft, ref writeRight); break;
#                 }}
#             }}
#
#             readRightV += N * (Unroll - 1);
#
#             //Console.WriteLine("WL:[{{string.Join(',', new Span<int>(left, (int)(writeLeft - left)).ToArray()}}]");
#             //Console.WriteLine("WR:[{{string.Join(',', new Span<int>(writeRight, (int)(right - writeRight)).ToArray()}}]");
#
#             while (readLeftV <= readRightV)
#             {{
#                 if (writeRight - readRightV < N)
#                 {{
#                     nextPtr = readRightV;
#                     readRightV -= N;
#                 }}
#                 else
#                 {{
#                     // PERF: Ensure that JIT never emits cmov here.
#                     nextPtr = readLeftV;
#                     readLeftV += N;
#                 }}
#
#                 default(MT).load_vec(nextPtr, out V d);
#                 partition_block(in d, in P, ref writeLeft, ref writeRight);
#             }}
#
#             //Console.WriteLine("WL:[{{string.Join(',', new Span<{t}>(left, (int)(writeLeft - left)).ToArray()}}]");
#             //Console.WriteLine("WR:[{{string.Join(',', new Span<{t}>(writeRight, (int)(right - writeRight)).ToArray()}}]");
#
#             // 3. Copy-back the 4 registers + remainder we partitioned in the beginning
#             var leftTmpSize = tmpLeft - tmpStartLeft;
#             Unsafe.CopyBlockUnaligned(writeLeft, tmpStartLeft, (uint)(leftTmpSize * sizeof(int)));
#             writeLeft += leftTmpSize;
#             //Console.WriteLine("WL:[{{string.Join(',', new Span<{t}>(left, (int)(writeLeft - left)).ToArray()}}]");
#             var rightTmpSize = tmpStartRight - tmpRight;
#             Unsafe.CopyBlockUnaligned(writeLeft, tmpRight, (uint)(rightTmpSize * sizeof(int)));
#             //writeLeft += rightTmpSize;
#
#             //Console.WriteLine("W:[{{string.Join(',', new Span<{t}>(left, (int)(right - left)).ToArray()}}]");
#             // Shove to pivot back to the boundary
#             *right = *writeLeft;
#             *writeLeft++ = pivot;
#
#             //Console.WriteLine("W:[{{string.Join(',', new Span<{t}>(left, (int)(right - left)).ToArray()}}]", ));
#
#             Debug.Assert(writeLeft > left);
#             Debug.Assert(writeLeft <= right + 1);
#
#             return writeLeft;
#         }}
#         """
#
#         print(s, file=f)
#
#
#     def generate_partitioning(self, f):
#         self.generate_partition_block(f)
#         self.generate_align_vectorized(f)
#         self.generate_partitioning_impl(f)


class VectorISA(Enum):
    AVX2 = 'AVX2'
    # AVX512 = 'AVX512'
    # SVE = 'SVE'

    def __str__(self):
        return self.value


def get_generator_supported_types(vector_isa):
    if isinstance(vector_isa, str):
        vector_isa = VectorISA[vector_isa]
    if vector_isa == VectorISA.AVX2:
        return AVX2SortingISA.supported_types()
    # elif vector_isa == VectorISA.AVX512:
    #     return AVX512SortingISA.supported_types()
    else:
        raise Exception(f"Non-supported vector machine-type: {vector_isa}")


def get_generator(vector_isa, type, configuration):
    if isinstance(vector_isa, str):
        vector_isa = VectorISA[vector_isa]
    if vector_isa == VectorISA.AVX2:
        return AVX2SortingISA(type, configuration)
    # elif vector_isa == VectorISA.AVX512:
    #     return AVX512BitonicISA(type)
    else:
        raise Exception(f"Non-supported vector machine-type: {vector_isa}")


def generate_per_type(f_header, type, vector_isa, configuration):
    g = get_generator(vector_isa, type, configuration)
    g.generate_prologue(f_header)
    g.generate_entry_points(f_header)
    g.generate_epilogue(f_header)


def generate_base_types(f_header, vector_isa, configuration):
    g = get_generator(vector_isa, type, configuration)
    g.generate_prologue(f_header)
    g.generate_master_entry_point(f_header)
    g.generate_epilogue(f_header)


def generate_all_types():
    parser = argparse.ArgumentParser()
    parser.add_argument("--vector-isa",
                        nargs='+',
                        default='all',
                        help='list of vector ISA to generate',
                        choices=list(VectorISA).append("all"))
    parser.add_argument("--output-dir", type=str, help="output directory")

    opts = parser.parse_args()

    if 'all' in opts.vector_isa:
        opts.vector_isa = list(VectorISA)

    config = Configuration()

    for isa in opts.vector_isa:
        filename = f"VectorizedSort.{isa}.generated"
        print(f"Generating {filename}.{{cs}}")
        h_filename = os.path.join(opts.output_dir, filename + ".cs")
        with open(h_filename, "w") as f_header:
            generate_base_types(f_header, isa, config)

        for t in get_generator_supported_types(isa):
            filename = f"VectorizedSort.{isa}.{t}.generated"
            print(f"Generating {filename}.{{cs}}")
            h_filename = os.path.join(opts.output_dir, filename + ".cs")
            with open(h_filename, "w") as f_header:
                generate_per_type(f_header, t, isa, config)


if __name__ == '__main__':
    generate_all_types()
